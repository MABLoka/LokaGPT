{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S_iMTmLXfLV",
        "outputId": "9908f86f-f5ba-4a6a-f6bb-b08191be77f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 1aJNW3CKrwduxL35tBIya6TbaoNWWQc8Y answers.txt\n",
            "Processing file 11KhbiY23sdpJkqzkIQQe4jLycPns6MdI prompts.txt\n",
            "Processing file 1iUbTcBJEHSbA9R9OfMwKr0Lvt5S4Hbeh train.txt\n"
          ]
        }
      ],
      "source": [
        "# download the tiny Trivial Q/A dataset from Google drive\n",
        "#\n",
        "!gdown --folder https://drive.google.com/drive/folders/1cLjPppGCbjL6w31F1l70Q8qtzL-8pRUb?usp=share_link 2> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSNVINviDGP5",
        "outputId": "6d12e5d1-7370-4807-bb12-e885fdb13b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "what was pierce brosnan's first outing as 007 [ goldeneye ]\n",
            "the 02 arena is in which london borough [ greenwich ]\n",
            "who wrote the 1956 novel '101 dalmatians' [ dodie smith ]\n",
            "which band's first top ten single was the 10538 overture in 1972 [ electric light orchestra ]\n",
            "the 1999 film 10 things i hate about you is based on which shakespeare play [ the taming of the shrew ]\n",
            "the film `10 things i hate about you` is based on which shakespeare play [ the taming of the shrew ]\n",
            "the film '10 things i hate about you', was inspired by which of shakespeare's plays [ the taming of the shrew ]\n",
            "who directed the 2010 film 127 hours [ danny boyle ]\n",
            "ciara had a hit with 1,2 step featuring which other artist [ missy elliot ]\n",
            "which film director won the oscar for best picture for the film 12 years a slave in 2013 [ steve mcqueen ]\n",
            "in which 2006 film does mark wahlberg try out for a university american football team [ invincible ]\n",
            "'the black and gold' is a nickname of which american football team [ pittsburgh steelers ]\n"
          ]
        }
      ],
      "source": [
        "# display the first few lines of training text, containing question and answer pairs\n",
        "\n",
        "!cat tinyQA/train.txt | head -n 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaAGyKMgDRSL",
        "outputId": "f7679c91-2b05-4719-d549-e6e43a95f0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "which football club did alan sugar own [\n",
            " 'the black and gold' is a nickname of which american football team [ \n",
            "alex band and aaron kamin make up which band [\n",
            "the aberdare mountains are in which african country [\n",
            "which notable leader won the 2009 nobel peace prize [\n",
            "who commanded the confederate army of northern virginia during the american civil war [ \n"
          ]
        }
      ],
      "source": [
        "# display the six questions as prompts\n",
        "\n",
        "!cat tinyQA/prompts.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuCBoXLoDaYo",
        "outputId": "97e4e710-f401-4ec7-fc8b-8088c7ee0acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "'the black and gold' is a nickname of which american football team [ pittsburgh steelers ]\n",
            "alex band and aaron kamin make up which band [ the calling ]\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "who commanded the confederate army of northern virginia during the american civil war [ robert e. lee ]\n"
          ]
        }
      ],
      "source": [
        "# display the questions and answers for all prompts\n",
        "\n",
        "!cat tinyQA/answers.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufHKQq9QNuS9",
        "outputId": "cc1a407a-57ad-457c-ba82-53f113b12063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters: 98,391\n",
            "all the unique characters: \n",
            " !#$%&'()*,-./0123456789:;[]_`abcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 57\n",
            "train has 88,551 tokens\n",
            "val has 9,840 tokens\n"
          ]
        }
      ],
      "source": [
        "# load text file as a character string; build char-level vocabulary, encoder and decoder\n",
        "#  (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "train_txt_file = 'tinyQA/train.txt'\n",
        "\n",
        "with open(train_txt_file, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JTchQzyGbc19"
      },
      "outputs": [],
      "source": [
        "# Define all parts in a GPT model, such as attention, MLP, LN modules\n",
        "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class config():\n",
        "  def __init__(self, batch_size=10, n_layer=12, n_head=12, n_embd=768, block_size=1024, vocab_size=50304, causal=True, device='cpu'):\n",
        "    assert n_embd %  n_head == 0\n",
        "    self.batch_size = batch_size\n",
        "    self.n_embd = n_embd\n",
        "    self.block_size = block_size\n",
        "    self.n_head = n_head\n",
        "    self.causal = causal\n",
        "    self.device = device\n",
        "    self.n_layer = n_layer\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).transpose(0,1)\n",
        "                            .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q.transpose(-2, -1) @ k) * (1.0 / math.sqrt(q.size(-2)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-2)\n",
        "        y = v @ att  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 3).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm([config.n_embd])\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm([config.n_embd])\n",
        "\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm([config.n_embd]),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        # Return the number of parameters in the model.\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lsy_tlbbz9a",
        "outputId": "5719434c-3b47-490f-9198-19b5e8ea605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 21.29M\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-7df8a51bfd57>:71: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 5.0748, val loss 5.0460 (time lapses 1.1134 seconds)\n",
            "step 1000: train loss 1.8632, val loss 2.0615 (time lapses 283.2819 seconds)\n",
            "step 2000: train loss 1.2955, val loss 2.0407 (time lapses 282.0643 seconds)\n",
            "step 3000: train loss 0.1965, val loss 4.0200 (time lapses 280.5803 seconds)\n",
            "step 4000: train loss 0.1178, val loss 4.8693 (time lapses 280.0938 seconds)\n",
            "step 5000: train loss 0.1035, val loss 5.2041 (time lapses 281.1528 seconds)\n",
            "step 6000: train loss 0.0954, val loss 5.4100 (time lapses 280.7283 seconds)\n",
            "step 7000: train loss 0.0848, val loss 5.4946 (time lapses 281.7341 seconds)\n",
            "step 8000: train loss 0.0838, val loss 5.6617 (time lapses 280.8972 seconds)\n",
            "step 9000: train loss 0.0831, val loss 5.7169 (time lapses 281.8602 seconds)\n",
            "step 10000: train loss 0.0708, val loss 5.9187 (time lapses 282.3412 seconds)\n"
          ]
        }
      ],
      "source": [
        "# training script to learn GPT models based on all given hyper-parameters\n",
        "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values\n",
        "# I/O\n",
        "\n",
        "######### model size ##########\n",
        "n_layer = 3      # num of layers\n",
        "n_head = 8       # num of attn heads per layer\n",
        "head_size = 96   # size of each attn head\n",
        "###############################\n",
        "\n",
        "###### training hyperparameters ######\n",
        "learning_rate = 6e-4   # max learning rate\n",
        "max_iters = 10000      # total number of training iterations\n",
        "batch_size = 24        # mini-batch size\n",
        "block_size = 256       # block size\n",
        "#####################################\n",
        "\n",
        "# adamw optimizer\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "eval_interval = 1000\n",
        "eval_iters = 200\n",
        "\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = n_head * head_size\n",
        "\n",
        "data_tr  = np.array(train_ids, dtype=np.uint16)\n",
        "data_val = np.array(val_ids, dtype=np.uint16)\n",
        "\n",
        "# poor man's data loader\n",
        "def get_batch(split):\n",
        "    data = data_tr if split == 'train' else data_val\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "conf = config(batch_size=batch_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd,\\\n",
        "              block_size=block_size, vocab_size=vocab_size, device=device)\n",
        "model = GPT(conf)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "iter_num = 0\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "\n",
        "    logits, loss = model(X, Y)\n",
        "    X, Y = get_batch('train')\n",
        "\n",
        "    # backward pass, with gradient scaling if training in fp16\n",
        "    scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} (time lapses {dt:.4f} seconds)\")\n",
        "\n",
        "    iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDqCrdeZbpJ8",
        "outputId": "39a15f0c-6f38-4d4d-daf0-9894162c6716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "---------------\n",
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "---------------\n",
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "---------------\n",
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "---------------\n",
            "which football club did alan sugar own [ tottenham hotspur f.c. ]\n",
            "---------------\n",
            " 'the black and gold' is a nickname of which american football team [ steve cross ]\n",
            "---------------\n",
            " 'the black and gold' is a nickname of which american football team [ aria ]\n",
            "---------------\n",
            " 'the black and gold' is a nickname of which american football team [ steve cross ]\n",
            "---------------\n",
            " 'the black and gold' is a nickname of which american football team [ suday ]\n",
            "---------------\n",
            " 'the black and gold' is a nickname of which american football team [ are ]\n",
            "---------------\n",
            "alex band and aaron kamin make up which band [ the calling ]\n",
            "---------------\n",
            "alex band and aaron kamin make up which band [ the calling ]\n",
            "---------------\n",
            "alex band and aaron kamin make up which band [ the calling ]\n",
            "---------------\n",
            "alex band and aaron kamin make up which band [ the calling ]\n",
            "---------------\n",
            "alex band and aaron kamin make up which band [ the carl lege ]\n",
            "---------------\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "---------------\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "---------------\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "---------------\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "---------------\n",
            "the aberdare mountains are in which african country [ kenya ]\n",
            "---------------\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "---------------\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "---------------\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "---------------\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "---------------\n",
            "which notable leader won the 2009 nobel peace prize [ barack obama ]\n",
            "---------------\n",
            "who commanded the confederate army of northern virginia during the american civil war [ michell cheller ]\n",
            "---------------\n",
            "who commanded the confederate army of northern virginia during the american civil war [ michell chell chef ]\n",
            "---------------\n",
            "who commanded the confederate army of northern virginia during the american civil war [ michell chell chef ]\n",
            "---------------\n",
            "who commanded the confederate army of northern virginia during the american civil war [ michell chell chellers ]\n",
            "---------------\n",
            "who commanded the confederate army of northern virginia during the american civil war [ michell chell chef ]\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# sample from a trained model to generate new text\n",
        "# (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import torch\n",
        "\n",
        "# start = \"\\n\"     # specify a text string as prompt\n",
        "start = \"FILE:tinyQA/prompts.txt\"  #can also specify a file of multiple prompts, use as: \"FILE:prompt.txt\"\n",
        "\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 100 # number of tokens generated in each sample\n",
        "temperature = 0.7 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = None #200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "\n",
        "seed = 1337\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# sampling the model based on one prompt\n",
        "def SampleOnePrompt(prompt):\n",
        "    start_ids = encode(prompt)\n",
        "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "    for k in range(num_samples):\n",
        "        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        res = decode(y[0].tolist())\n",
        "        answers = res.splitlines()\n",
        "        print(answers[0])              # only show the first line for QA\n",
        "\n",
        "        # sample = decode(y[0].tolist())\n",
        "        # sampleAns = sample[:sample.index(']')+1]\n",
        "        # print(sampleAns)    # show all sampling results\n",
        "        print('---------------')\n",
        "\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):     # for multiple prompts\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        Lines = f.readlines()\n",
        "        for prompt in Lines:\n",
        "            SampleOnePrompt(prompt[:-1])  # chop the trailing newline char\n",
        "else:\n",
        "     SampleOnePrompt(start)     # for one prompt\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
